{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHQsFAwfcK9u"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key = 'REMOVED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQaI0BXpnGoM"
   },
   "source": [
    "### 1 - obtain embedding from openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1725228779969,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "8kb_49a1ch4W",
    "outputId": "53a65959-4675-4efa-d0bb-6c45a6b1c38c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.023360375314950943,\n",
       " -0.003970523364841938,\n",
       " 0.016770603135228157,\n",
       " -0.015610604546964169,\n",
       " -0.00869998149573803,\n",
       " -0.04541266709566116,\n",
       " 0.03689779341220856,\n",
       " 0.04570883885025978,\n",
       " 0.008459343574941158,\n",
       " 0.0014577097026631236,\n",
       " -0.0026146220043301582,\n",
       " -0.025668030604720116,\n",
       " -0.024619096890091896,\n",
       " -0.04210544005036354,\n",
       " -0.04185863211750984,\n",
       " -0.009761255234479904,\n",
       " 0.0004720202705357224,\n",
       " 0.01091508287936449,\n",
       " -0.01520337164402008,\n",
       " 0.023520801216363907,\n",
       " 0.009600830264389515,\n",
       " -0.0699947401881218,\n",
       " 0.005624136887490749,\n",
       " 0.019621234387159348,\n",
       " 0.029123341664671898,\n",
       " -0.008366790600121021,\n",
       " -0.008317428641021252,\n",
       " 0.030085893347859383,\n",
       " 0.05617349594831467,\n",
       " -0.02168208174407482]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"text-embedding-3-small\"\n",
    "\n",
    "result = client.embeddings.create(\n",
    "    input = ['This is a test input, we want embeddings!'],\n",
    "    model=model)\n",
    "result.data[0].embedding[:30] # resulting one list of embedding, deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1725228246707,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "yUFW25AJg6aj",
    "outputId": "cdd438d3-2dbb-4a57-9824-a5cdc99b4d70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(result.data[0].embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1725228708540,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "N_ArOdfUnONd",
    "outputId": "5f2e9585-0503-4947-cc66-7c8393efc9f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = client.embeddings.create(\n",
    "    input = ['This is another a test input with a longer length, will it return longer embeddings?'],\n",
    "    model=model)\n",
    "np.array(result.data[0].embedding).shape\n",
    "\n",
    "# always return embeddings with 1536 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1725228786297,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "Ohoi87I9pHHT",
    "outputId": "f23481a1-d74a-4da8-f3b1-d191368e5319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011657672002911568,\n",
       " 0.007152515463531017,\n",
       " -0.0012050794903188944,\n",
       " -0.0253323782235384,\n",
       " 0.005374339409172535,\n",
       " -0.03888102248311043,\n",
       " -0.01372115220874548,\n",
       " 0.03014937974512577,\n",
       " 0.037686724215745926,\n",
       " 0.022107776254415512,\n",
       " 0.033705733716487885,\n",
       " 0.0058122482150793076,\n",
       " -0.03893410041928291,\n",
       " -0.01633533649146557,\n",
       " -0.0050691296346485615,\n",
       " 0.018418723717331886,\n",
       " -0.035536989569664,\n",
       " 0.005072447471320629,\n",
       " -0.044905588030815125,\n",
       " 0.01385385263711214,\n",
       " 0.015114499256014824,\n",
       " -0.01845853216946125,\n",
       " 0.04291509464383125,\n",
       " 0.01326333824545145,\n",
       " 0.06003335863351822,\n",
       " -0.05806940421462059,\n",
       " -0.04817000404000282,\n",
       " 0.004654443357139826,\n",
       " 0.046710304915905,\n",
       " -0.034156911075115204]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = client.embeddings.create(\n",
    "    input = ['This is a test input, ', 'we want embeddings!'],\n",
    "    model=model)\n",
    "result.data[0].embedding[:30]\n",
    "# break apart gives separate embedding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725230965902,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "AB5w7DHlxenQ",
    "outputId": "327bc1cd-23ff-4114-db1e-9e2558576b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01195084024220705,\n",
       " -0.0028240245301276445,\n",
       " -0.01856301538646221,\n",
       " 0.007554715033620596,\n",
       " -0.016591593623161316,\n",
       " -0.03105347603559494,\n",
       " 0.053098853677511215,\n",
       " 0.028276219964027405,\n",
       " -0.04506927356123924,\n",
       " -0.026103340089321136,\n",
       " -0.016174284741282463,\n",
       " -0.037787966430187225,\n",
       " 0.004378137178719044,\n",
       " -0.047314103692770004,\n",
       " -0.010799645446240902,\n",
       " -0.012029984965920448,\n",
       " 0.04127033054828644,\n",
       " -0.03272270783782005,\n",
       " 0.017167190089821815,\n",
       " 0.022808045148849487,\n",
       " 0.00974917970597744,\n",
       " -0.049127232283353806,\n",
       " 0.00048431119648739696,\n",
       " 0.027125025168061256,\n",
       " -0.003394225612282753,\n",
       " 0.04196104779839516,\n",
       " 0.007011495064944029,\n",
       " 0.021584900096058846,\n",
       " 0.05393347144126892,\n",
       " -0.012526437640190125]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.data[1].embedding[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfSdbZOZfV6e"
   },
   "source": [
    "### 2 - Vector Database with Pinecone\n",
    "1. data base of embeddings of documents, possible replacement for certain fine-tuning purposes\n",
    "2. documentation: https://www.pinecone.io/learn/openai-gen-qa/\n",
    "3. search process\n",
    "    - user ask query\n",
    "    - query convert to embedding\n",
    "    - compare against all embeddings of documents\n",
    "    - the dot product with highest values return as results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HhMb_ueojwV"
   },
   "outputs": [],
   "source": [
    "# download dataset and pinecone-client API\n",
    "!pip install -qU openai pinecone-client datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2140,
     "status": "ok",
     "timestamp": 1725233297909,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "t20pNgBN29Cs",
    "outputId": "e7d970fc-4c31-4b8a-8a71-8bf18a6ca4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n",
      "    num_rows: 208619\n",
      "})\n",
      "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'published': '2021-07-06 13:00:03 UTC', 'url': 'https://youtu.be/35Pdoyi6ZoQ', 'video_id': '35Pdoyi6ZoQ', 'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'id': '35Pdoyi6ZoQ-t0.0', 'text': 'Hi, welcome to the video.', 'start': 0.0, 'end': 9.36}\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset, transcription from videos that could contain answer\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('jamescalam/youtube-transcriptions', split='train')\n",
    "print(data)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "ef7409753e17486e96246c5dad2e4b03",
      "cf015341a6224e518f082f7f91c28d6d",
      "b2b6ca72b80e46848f2a56e615c3bbef",
      "9951535f583f44d0817da251d3cf6ded",
      "bb4337533a5c4de1bc47a2b0de944c70",
      "b3f335d9e3eb42209997dec316ae738d",
      "368e095c405e474183d863d71279fe75",
      "f3bd05774b7b48ec9b675f98d5986cba",
      "a451137b9ef04f94891b3aade3434816",
      "4700f8c877c04c57b773d667f6f14ac5",
      "e2c6949c87f0444084672d1416bf2f48"
     ]
    },
    "executionInfo": {
     "elapsed": 110840,
     "status": "ok",
     "timestamp": 1725233960765,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "ZivbWixP6lE4",
    "outputId": "401b34a8-6075-438b-d87b-3f61184b7a08"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7409753e17486e96246c5dad2e4b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "combine every 20 sentences into a single sentence for embedding\n",
    "'''\n",
    "new_data = []\n",
    "\n",
    "window = 20 # number of sentences to combine\n",
    "stride = 4 # number of sentences to stride over\n",
    "\n",
    "for i in range(0, len(data), stride):\n",
    "    j = min(len(data) - 1, i + window)\n",
    "\n",
    "    # don't encompass two videos\n",
    "    if data[i]['title'] != data[j]['title']:\n",
    "        continue\n",
    "\n",
    "    text = ' '.join(data[i:j]['text'])\n",
    "\n",
    "    new_data.append({\n",
    "        'start': data[i]['start'],\n",
    "        'end': data[j]['end'],\n",
    "        'title': data[i]['title'],\n",
    "        'text': text,\n",
    "        'id': data[i]['id'],\n",
    "        'url': data[i]['url'],\n",
    "        'published': data[i]['published'],\n",
    "        'channel_id': data[i]['channel_id']\n",
    "    })\n",
    "\n",
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2860,
     "status": "ok",
     "timestamp": 1725236967359,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "RFvvxSR69fJH",
    "outputId": "8de944f6-aea0-443b-978a-d86e7d90f5a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone_client = pinecone.Pinecone(api_key = \"4c13b39e-9c6a-4830-ac5e-3f2cd4d1ad14\")\n",
    "index_name = 'study-vector-database'\n",
    "\n",
    "pinecone_client.create_index(\n",
    "    index_name,\n",
    "    dimension = 1536, # embedding size = 1536 = len(result.data[0].embedding)\n",
    "    metric='cosine', # comparing metric\n",
    "    spec = pinecone.ServerlessSpec(\n",
    "        cloud = 'aws',\n",
    "        region = 'us-east-1'\n",
    "    )\n",
    ")\n",
    "\n",
    "index = pinecone_client.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aj2Zq5MrIX0J"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # show progress bar\n",
    "from time import sleep\n",
    "# create massive embeddings, and upload to cloud\n",
    "batch_size = 50\n",
    "\n",
    "with tqdm(total = int(len(new_data) * .1)) as pbar:\n",
    "    for i in range(0, int(len(new_data) * .1), batch_size):\n",
    "        j = min(len(new_data), i + batch_size)\n",
    "        batch = new_data[i: j]\n",
    "        idxs = [x['id'] for x in batch]\n",
    "        texts = [x['text'] for x in batch]\n",
    "        try:\n",
    "            completions = client.embeddings.create(model = 'text-embedding-3-small',\n",
    "                                                   input = texts)\n",
    "        except: # rate limit, then wait until clear\n",
    "            done = False\n",
    "            while not done:\n",
    "                sleep(5)\n",
    "                try:\n",
    "                    completions = client.embeddings.create(model = 'text-embedding-3-small',\n",
    "                                                           input = texts)\n",
    "                    done = True\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        embedds = [completions.data[x].embedding for x in range(len(completions.data))]\n",
    "        meta_batch = [{\n",
    "            'start': x['start'],\n",
    "            'end': x['end'],\n",
    "            'title': x['title'],\n",
    "            'text': x['text'],\n",
    "            'url': x['url'],\n",
    "            'published': x['published'],\n",
    "            'channel_id': x['channel_id']\n",
    "        } for x in batch]\n",
    "\n",
    "        to_upsert = list(zip(idxs, embedds, meta_batch))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZHtIB3kW-Qh"
   },
   "outputs": [],
   "source": [
    "# query index from the database\n",
    "query = \"Where to go for instructions to install torch?\"\n",
    "query_embed = client.embeddings.create(model = 'text-embedding-3-small',\n",
    "                                       input = [query]).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1813,
     "status": "ok",
     "timestamp": 1725241865956,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "leWUDesybZf2",
    "outputId": "6356c352-be9e-44e4-f5c8-5fe983f1df7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'uYas6ysyjgY-t387.0',\n",
       "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
       "                           'end': 597.0,\n",
       "                           'published': '2022-05-24 13:00:34 UTC',\n",
       "                           'start': 387.0,\n",
       "                           'text': \"And you'll probably see this where it \"\n",
       "                                   'says, please reactivate your environment. '\n",
       "                                   'So to do that, we just do conda activate '\n",
       "                                   'and run that. That switches back to the '\n",
       "                                   'base environment. And then we literally '\n",
       "                                   'just activate ML again. Now, the next step '\n",
       "                                   'is to actually pip install PyTorch. And to '\n",
       "                                   'do that, we are doing what you can see '\n",
       "                                   'here. So I am running pip install upgrade. '\n",
       "                                   'You might not need the upgrade flag there, '\n",
       "                                   'but just in case. And we need to make sure '\n",
       "                                   'we are going to install the nightly '\n",
       "                                   'version of PyTorch because as of this '\n",
       "                                   'moment, version 1.12 is only released in '\n",
       "                                   'the nightly releases, which is basically '\n",
       "                                   'just a more frequent but slightly more '\n",
       "                                   'unstable releases that PyTorch releases. '\n",
       "                                   'So you need all of this here. So go across '\n",
       "                                   \"and you'll be able to copy these from the \"\n",
       "                                   'notebook link that I have in the '\n",
       "                                   'description below. OK, so we run that. And '\n",
       "                                   'one thing to just be aware of here. So if '\n",
       "                                   \"we have a look here, you know it's working \"\n",
       "                                   \"and it's being installed in the correct \"\n",
       "                                   'version. If you can see that it says ARM64 '\n",
       "                                   'up here. Now, if you are just wanting to '\n",
       "                                   \"use PyTorch with NPS, that's ready. You \"\n",
       "                                   \"can go ahead and start using it and I'll \"\n",
       "                                   'show you how in a moment. But for those of '\n",
       "                                   'you that are going to want to use Houdini '\n",
       "                                   'Placed Transformers, there is an extra '\n",
       "                                   'step. If we try and pip install '\n",
       "                                   'Transformers datasets, this will probably '\n",
       "                                   'come up with an error for most of you. For '\n",
       "                                   'me, because I have already dealt with the '\n",
       "                                   \"error, it's not popping up and it would \"\n",
       "                                   'say something like error failed building '\n",
       "                                   'wheels for tokenizers. So the reason for '\n",
       "                                   'that is Transformers tokenizers have '\n",
       "                                   'particular tokenizers that are faster than '\n",
       "                                   'other tokenizers and they are faster '\n",
       "                                   'because they use Rust. Now, Rust is not '\n",
       "                                   'installed from the ARM distribution of '\n",
       "                                   'Python that we have at the moment. So from '\n",
       "                                   'within our new environment, all we need to '\n",
       "                                   'do is install Rust like this. Once we '\n",
       "                                   'execute that, we can just go ahead and pip '\n",
       "                                   'install Transformers and datasets like we '\n",
       "                                   'did before. And with that, we are ready to '\n",
       "                                   'move on to our code. So for this example, '\n",
       "                                   'all we need are these libraries here with '\n",
       "                                   'Torch Transformers datasets, which we '\n",
       "                                   'already installed. And we can check that '\n",
       "                                   'our PyTorch installation has NPS using '\n",
       "                                   'this. So Torch has NPS. If you run that, '\n",
       "                                   \"you should see true. And that means you're \"\n",
       "                                   \"OK and you're ready to go with the rest of \"\n",
       "                                   \"the code. So here I'm just pulling some \"\n",
       "                                   \"data. I'm not really going to go into \"\n",
       "                                   'detail on this.',\n",
       "                           'title': 'New GPU-Acceleration for PyTorch on M1 '\n",
       "                                    'Macs! + using with BERT',\n",
       "                           'url': 'https://youtu.be/uYas6ysyjgY'},\n",
       "              'score': 0.439408034,\n",
       "              'values': []},\n",
       "             {'id': 'uYas6ysyjgY-t344.0',\n",
       "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
       "                           'end': 565.0,\n",
       "                           'published': '2022-05-24 13:00:34 UTC',\n",
       "                           'start': 344.0,\n",
       "                           'text': 'OK, so once that has installed, we need to '\n",
       "                                   'go into that environment. So conda '\n",
       "                                   'activate ML. And then we need to '\n",
       "                                   'permanently modify the conda subdirectory '\n",
       "                                   'variable to make sure that this is always '\n",
       "                                   'going to be set to OS X ARM 64. So this is '\n",
       "                                   'to avoid later on when we start pip '\n",
       "                                   'installing things in this environment. '\n",
       "                                   'This variable may switch back to x86, '\n",
       "                                   \"which we don't want. So we need to make \"\n",
       "                                   'sure it stays with the ARM environment '\n",
       "                                   'architecture. So we add that in there. And '\n",
       "                                   \"you'll probably see this where it says, \"\n",
       "                                   'please reactivate your environment. So to '\n",
       "                                   'do that, we just do conda activate and run '\n",
       "                                   'that. That switches back to the base '\n",
       "                                   'environment. And then we literally just '\n",
       "                                   'activate ML again. Now, the next step is '\n",
       "                                   'to actually pip install PyTorch. And to do '\n",
       "                                   'that, we are doing what you can see here. '\n",
       "                                   'So I am running pip install upgrade. You '\n",
       "                                   'might not need the upgrade flag there, but '\n",
       "                                   'just in case. And we need to make sure we '\n",
       "                                   'are going to install the nightly version '\n",
       "                                   'of PyTorch because as of this moment, '\n",
       "                                   'version 1.12 is only released in the '\n",
       "                                   'nightly releases, which is basically just '\n",
       "                                   'a more frequent but slightly more unstable '\n",
       "                                   'releases that PyTorch releases. So you '\n",
       "                                   'need all of this here. So go across and '\n",
       "                                   \"you'll be able to copy these from the \"\n",
       "                                   'notebook link that I have in the '\n",
       "                                   'description below. OK, so we run that. And '\n",
       "                                   'one thing to just be aware of here. So if '\n",
       "                                   \"we have a look here, you know it's working \"\n",
       "                                   \"and it's being installed in the correct \"\n",
       "                                   'version. If you can see that it says ARM64 '\n",
       "                                   'up here. Now, if you are just wanting to '\n",
       "                                   \"use PyTorch with NPS, that's ready. You \"\n",
       "                                   \"can go ahead and start using it and I'll \"\n",
       "                                   'show you how in a moment. But for those of '\n",
       "                                   'you that are going to want to use Houdini '\n",
       "                                   'Placed Transformers, there is an extra '\n",
       "                                   'step. If we try and pip install '\n",
       "                                   'Transformers datasets, this will probably '\n",
       "                                   'come up with an error for most of you. For '\n",
       "                                   'me, because I have already dealt with the '\n",
       "                                   \"error, it's not popping up and it would \"\n",
       "                                   'say something like error failed building '\n",
       "                                   'wheels for tokenizers. So the reason for '\n",
       "                                   'that is Transformers tokenizers have '\n",
       "                                   'particular tokenizers that are faster than '\n",
       "                                   'other tokenizers and they are faster '\n",
       "                                   'because they use Rust. Now, Rust is not '\n",
       "                                   'installed from the ARM distribution of '\n",
       "                                   'Python that we have at the moment. So from '\n",
       "                                   'within our new environment, all we need to '\n",
       "                                   'do is install Rust like this. Once we '\n",
       "                                   'execute that, we can just go ahead and pip '\n",
       "                                   'install Transformers and datasets like we '\n",
       "                                   'did before. And with that, we are ready to '\n",
       "                                   'move on to our code.',\n",
       "                           'title': 'New GPU-Acceleration for PyTorch on M1 '\n",
       "                                    'Macs! + using with BERT',\n",
       "                           'url': 'https://youtu.be/uYas6ysyjgY'},\n",
       "              'score': 0.422621161,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = index.query(vector = query_embed, top_k = 2, include_metadata = True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3321,
     "status": "ok",
     "timestamp": 1725243020336,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "8E--IPEuWcT_",
    "outputId": "9213f239-d29e-4f77-9d25-521b82b89cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For instructions on how to install torch, you should refer to the notebook link provided in the description mentioned in the context. The context explains that the installation commands and steps can be copied from this notebook link.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this result can be then fed into prompt engineering queries\n",
    "# based on the context to provide better results\n",
    "def complete(query, limit = 3750):\n",
    "    res = client.embeddings.create(\n",
    "        input=[query],\n",
    "        model='text-embedding-3-small').data[0].embedding\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(vector=res, top_k=3, include_metadata=True)\n",
    "    contexts = [x['metadata']['text'] for x in res['matches']]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt = (\n",
    "        \"Task\\n---\\nAnswer the Question based on the context below.\\n\\n\"+\n",
    "        f\"QuestionTask\\n---\\n{query}\"+\n",
    "        \"Context\\n---\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for context in contexts:\n",
    "        prompt = prompt + context + '\\n'\n",
    "        if(len(prompt) >= limit):\n",
    "            break\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "            model = 'gpt-4o',\n",
    "            messages = [{'role': 'user', 'content': prompt}]\n",
    "        ).choices[0].message.content\n",
    "\n",
    "complete(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aORNVsNRwYkT"
   },
   "source": [
    "### 3 - Reason and Act, Chatbot OOP Implementation\n",
    "to restrain deviation, one can define actions and parse, use functions to produce deterministic results for AI to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTcg9QkIy8tZ"
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY9NAvLjzEIh"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "class ChatBot:\n",
    "    def __init__(self, client, system_message = \"\"):\n",
    "        self.client = client\n",
    "        self.messages = []\n",
    "        if system_message:\n",
    "            self.messages.append({'role': 'system', 'content': system_message})\n",
    "\n",
    "    def __call__(self, message = ''):\n",
    "        self.messages.append({'role': 'user', 'content': message})\n",
    "        assistant_message = self.complete()\n",
    "        self.messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "        return assistant_message\n",
    "\n",
    "    def complete(self):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model = 'gpt-4o',\n",
    "            messages = self.messages)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bobVkY_-zZoi"
   },
   "outputs": [],
   "source": [
    "# this will produce a loop, the bot might acquire several rounds of information\n",
    "prompt = \"\"\"\n",
    "Task\n",
    "---\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions. Run another loop if you need more information.\n",
    "Return Answer Immediately if having enough information\n",
    "---\n",
    "\n",
    "Action\n",
    "---\n",
    "calculate: {{expression}}\n",
    "low priority, runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "wikipedia: {{term}}\n",
    "High priority, returns a summary from searching Wikipedia\n",
    "---\n",
    "\n",
    "Example Session\n",
    "---\n",
    "Question: What is the capital of France?\n",
    "Thought: I should look up France on Wikipedia\n",
    "Action: wikipedia: France\n",
    "PAUSE\n",
    "\n",
    "You then will be called again with Observation, and you will return your answer based on your thoughts about the Observation:\n",
    "DO NOT RETURN ANSWER IF NOT GIVEN OBSERVATION\n",
    "IF IT IS NUMBER, ONLY RETURN THE NUMBER AFTER ANSWER\n",
    "Observation: France is a country, and the capital is Paris\n",
    "Answer: The capital of France is Paris\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsPrqcDg00A0"
   },
   "outputs": [],
   "source": [
    "# define parsing and each action\n",
    "action_pattern = re.compile(r'^Action: (\\w+): (.*)$')\n",
    "\n",
    "def wikipedia(query):\n",
    "    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"format\": \"json\"\n",
    "    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
    "\n",
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "action_dict = {\n",
    "    'wikipedia': wikipedia,\n",
    "    'calculate': calculate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH-06bxBNjRc"
   },
   "outputs": [],
   "source": [
    "# ensure the consistency of format for parsing\n",
    "test_prompts = ['what is happiness?', 'define pathos and ethos', 'what is 3 * 2 + 7?',\n",
    "                'what is capital of China?', 'what is 7 * 7?']\n",
    "observation = ['a state of well-being and contentment : joy', 'Pathos An emotional appeal that uses vivid language, sensory images, and anecdotes to evoke feelings in the audience. Pathos can help the audience understand how an argument will affect the real world. Ethos An ethical appeal that establishes the writer\\'s credibility and authority. Ethos can be conveyed by stating one\\'s occupation and experience, or by using first-person plural pronouns like \\\"we\\\" and \\\"us\\\".',\n",
    "               '13', 'China is an Asian country, its\\' capital is Beijing', '49']\n",
    "for i in range(len(test_prompts)):\n",
    "    print(f'------TRIAL {i + 1}------')\n",
    "    bot = ChatBot(client, prompt)\n",
    "    print(bot(test_prompts[i]))\n",
    "    print(bot(observation[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T05Da3a2St3E"
   },
   "outputs": [],
   "source": [
    "def query(question, client = client, prompt = prompt, max_turns = 5):\n",
    "    bot = ChatBot(client, prompt)\n",
    "    next_prompt = bot(question)\n",
    "    for i in range(max_turns):\n",
    "        print(next_prompt)\n",
    "        actions = [re.match(r'^Action: (\\w+): (.*)$', a) for a in next_prompt.split('\\n') if re.match(r'^Action: (\\w+): (.*)$', a)]\n",
    "        if actions:\n",
    "            action, item = actions[0].groups()\n",
    "            if action not in action_dict:\n",
    "                return \"unknown action generated, terminate\"\n",
    "            print(f\"---running {action} {item}\")\n",
    "            next_prompt = \"Observation: \" + str(action_dict[action](item))\n",
    "            print(next_prompt)\n",
    "            next_prompt = bot(next_prompt)\n",
    "        else:\n",
    "            return\n",
    "    return bot('return result immediately')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71095,
     "status": "ok",
     "timestamp": 1725310543296,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "BvgkxB3ocu7U",
    "outputId": "9209cc46-9ed2-45a6-ab44-c405874008f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Let's look up China on Wikipedia to find out its capital city.\n",
      "Action: wikipedia: China\n",
      "PAUSE\n",
      "---running wikipedia China\n",
      "Observation: <span class=\"searchmatch\">China</span>, officially the People's Republic of <span class=\"searchmatch\">China</span> (PRC), is a country in East Asia. With a population exceeding 1.4 billion, it is the world's second-most\n",
      "Action: wikipedia: People's Republic of China\n",
      "PAUSE\n",
      "---running wikipedia People's Republic of China\n",
      "Observation: <span class=\"searchmatch\">China</span>, officially the <span class=\"searchmatch\">People's</span> <span class=\"searchmatch\">Republic</span> <span class=\"searchmatch\">of</span> <span class=\"searchmatch\">China</span> (PRC), is a country in East Asia. With a population exceeding 1.4 billion, it is the world's second-most\n",
      "Thought: The search results provide information about China but not specifically about its capital. I will refine my search to directly look for \"Capital of China\".\n",
      "Action: wikipedia: Capital of China\n",
      "PAUSE\n",
      "---running wikipedia Capital of China\n",
      "Observation: seat <span class=\"searchmatch\">of</span> the Nationalist government <span class=\"searchmatch\">of</span> the Republic <span class=\"searchmatch\">of</span> <span class=\"searchmatch\">China</span> in late 1949 towards the end <span class=\"searchmatch\">of</span> the <span class=\"searchmatch\">Chinese</span> Civil War. Chongqing was the <span class=\"searchmatch\">capital</span> city <span class=\"searchmatch\">of</span> Ba\n",
      "Thought: The search results mention Chongqing but do not directly indicate the current capital of China. I will try another search with a more specific term.\n",
      "Action: wikipedia: Beijing\n",
      "PAUSE\n",
      "---running wikipedia Beijing\n",
      "Observation: In 2020, the <span class=\"searchmatch\">Beijing</span> subway was the fourth busiest and second longest in the world. The <span class=\"searchmatch\">Beijing</span> Daxing International Airport, <span class=\"searchmatch\">Beijing's</span> second international\n",
      "Answer: The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "query('what is capital of China?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1725310709167,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "yYljE02ehieB",
    "outputId": "783bb281-5d91-4f31-c554-727b57b9ab95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: To understand why college football is so popular, I should look up a summary of the topic which will provide insights into the cultural, historical, and societal aspects that contribute to its popularity.\n",
      "\n",
      "Action: wikipedia: College football\n",
      "PAUSE\n",
      "---running wikipedia College football\n",
      "Observation: <span class=\"searchmatch\">College</span> <span class=\"searchmatch\">football</span> is gridiron <span class=\"searchmatch\">football</span> that is played by teams of amateur student-athletes at universities and <span class=\"searchmatch\">colleges</span>. It was through collegiate competition\n",
      "Answer: College football is popular because it involves amateur student-athletes from universities and colleges, creating a unique blend of competition, school spirit, regional pride, and deep-rooted traditions that resonate with students, alumni, and local communities.\n"
     ]
    }
   ],
   "source": [
    "query('why is collge football so popular?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8ykU5VlifrF"
   },
   "source": [
    "### 4 - Recursive Re-prompting and Revision\n",
    "1. Break down tasks into smaller ones when writig things; first create characters and plot;\n",
    "2. then create drafts, each draft is a passage for a plot point (one split the plot into several plot points); to generate passage, give the previous point and the later point\n",
    "3. in the end revise by lettting the model look back at itself; for each passage, let it check whether there are inconsistencies with the front and back, and fix it self. Do it no inconsistencies found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1725319464162,
     "user": {
      "displayName": "Maohe Jiang",
      "userId": "14592769344524725558"
     },
     "user_tz": 420
    },
    "id": "A-hP9Ss2ie8Q",
    "outputId": "d3f7b3be-9d20-4bb4-9815-8fbc92fd6c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, it appears to be working. How can I assist you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete(prompt, stop=None):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "complete('is this working?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xiiX5FW--VD"
   },
   "outputs": [],
   "source": [
    "# Planning\n",
    "premise_input = \"Premise: A new law grad returns home to start her career, but struggles with the broken justice system.\"\n",
    "\n",
    "# Generate Setting\n",
    "setting_prompt = f\"{premise}\\nThe story is set in\"\n",
    "setting_response = complete(setting_prompt, stop=[\".\"])\n",
    "setting = f\"Setting: The story is set in {setting_response}.\"\n",
    "print(setting)\n",
    "\n",
    "# Generate Characters\n",
    "num_characters = 2\n",
    "characters = \"\"\n",
    "character_format = \"Please invent characters for this story, in the format of [name] is [context]. For example 'Liza Turner is a 22-year-old woman' or 'Peyton Turner is Liza's older sister'. Each character is distinct and does not reuse the same name as these examples.\"\n",
    "character_prompt = f\"Premise: {premise}\\n\\nSetting: {setting}\\n\\n{character_format}\\n\\n\"\n",
    "for n in range(num_characters):\n",
    "    character_prompt += f\"{n+1}. Character Portrait:\\n\"\n",
    "    character_response = complete(f\"{character_prompt}\\n\\n{n+1}. Character Portrait:\\n\", stop=[\".\"])\n",
    "    character_prompt += f\"{character_response}\\n\\n\"\n",
    "    characters += f\"{n+1}. Character Portrait:\\n{character_response.strip()}.\\n\\n\"\n",
    "print(characters)\n",
    "\n",
    "# plot\n",
    "plot_prompt = f\"{premise}\\n\\n{setting}\\n\\n{characters}Outline the main plot points of the story.\\n1.\"\n",
    "plot_response = complete(plot_prompt)\n",
    "plot = f\"{plot_response}\"\n",
    "print(\"Outline the main plot points of the story\\n\\n1. \", plot)\n",
    "\n",
    "# entire planning\n",
    "print(f\"{premise}\\n\\n{setting}\\n\\n{characters}\\n\\nOutline the main plot points of the story\\n\\n1. {plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NanJ-5gkDyLZ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "plot_points_raw = plot.split('\\n')\n",
    "plot_points = []\n",
    "\n",
    "for pp in plot_points_raw:\n",
    "    if pp == '':\n",
    "        continue\n",
    "    else:\n",
    "        stripped_pp = re.sub(r'^\\d+\\.\\s*', '', pp)\n",
    "        plot_points.append(stripped_pp)\n",
    "\n",
    "print(plot_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxkqQNTtDzJm"
   },
   "outputs": [],
   "source": [
    "drafts = []\n",
    "\n",
    "for idx in range(len(plot_points)):\n",
    "    if idx == 0:\n",
    "        previous = \"\"\n",
    "        immediately = \"\"\n",
    "    else:\n",
    "        previous = f\"Previous story summary:\\n{plot_points[idx-1]}\"\n",
    "        immediately = f\"Immediately before the current passage:\\n{drafts[idx-1]}\"\n",
    "\n",
    "    upcoming = f\"In the upcoming passage,\\n{plot_points[idx]}\"\n",
    "\n",
    "    draft_prompt = f\"Relevant context:\\n{characters}{previous}\\n\\n{immediately}\\n\\n{upcoming}\\n\\nFull text below:\"\n",
    "\n",
    "    draft_response = complete(draft_prompt)\n",
    "    draft_response = draft_response.strip()\n",
    "\n",
    "    print(f\"Plot Point {idx+1}\\n-----\\n\", draft_prompt, f\"\\n{draft_response}\\n-----\\n\\n\")\n",
    "    drafts.append(draft_response)\n",
    "\n",
    "print(\"\".join(drafts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2hk7qsND2yd"
   },
   "outputs": [],
   "source": [
    "# Revision\n",
    "# Progressive extraction\n",
    "facts = []\n",
    "edits = []\n",
    "for idx in range(1, len(drafts)):\n",
    "    # Old Facts\n",
    "    facts_prompt = f\"What facts can be inferred from this text?\\n\\nText:\\n{drafts[idx-1]}\\n\\nFacts:\\n-\"\n",
    "    facts_response = complete(facts_prompt)\n",
    "    facts_response = facts_response.strip()\n",
    "    facts.append(facts_response)\n",
    "\n",
    "    # New Facts\n",
    "    facts_prompt = f\"What facts can be inferred from this text?\\n\\nText:\\n{drafts[idx]}\\n\\nFacts:\\n-\"\n",
    "    facts_response = complete(facts_prompt)\n",
    "    facts_response = facts_response.strip()\n",
    "\n",
    "    # Consistency, also input true or false\n",
    "    joined_facts = \"\\n\".join(facts)\n",
    "    consistency_prompt = f\"Are any New Facts inconsistent with Old Facts?:\\n\\Old facts:\\n{joined_facts}\\n\\nNew facts:{facts_response}\\n\\nInconsistencies:\\n-\"\n",
    "    consistency_response = complete(consistency_prompt)\n",
    "\n",
    "    # Rewriting, can be in a while loop with consistency until consistent\n",
    "    edit_prompt = f\"Edit so that any inconsistencies are addressed:\\nInconsistencies:{consistency_response}\\n\\nText:{drafts[idx]}\\n\\nRewritten Text:\"\n",
    "    edit_response = complete(edit_prompt)\n",
    "    edit_response = edit_response.strip()\n",
    "    edits.append(edit_response)\n",
    "\n",
    "    print(f\"Draft Text {idx+1}\\n-----\\n\", drafts[idx], f\"\\n\\nOld Facts:\\n{joined_facts}\", f\"\\n\\nNew Facts:\\n{facts_response}\", f\"\\n\\nConsistency:\\n{consistency_response}\", f\"\\n\\nRewritten Text:\\n{edit_response}\\n-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Fm92RhhExT6"
   },
   "source": [
    "### 5 - Vector Database with Faiss, an open source local library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSCV3M2tKi6B"
   },
   "outputs": [],
   "source": [
    "def get_vector_embeddings(text, client = client):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "get_vector_embeddings(\"Your text string goes here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHh4EJcYKn-A"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_text(text, window_size=5, stride=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = re.split('(?<=[.!?]) +', text.strip())\n",
    "\n",
    "    # Initialize the list of chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Slide the window over the sentences\n",
    "    for i in range(0, len(sentences) - window_size + 1, stride):\n",
    "        # Add the current window to the chunks\n",
    "        chunks.append(' '.join(sentences[i:i + window_size]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "text = \"\"\"\n",
    "From Adam West to Robert Pattinson: this is how Batman has changed over the years\n",
    "Batman is one of the most recognizable superheroes in pop culture, starting with his first appearance in Detective Comics #27 in 1939. He has been listed as one of the most iconic characters in popular culture, and among the greatest comic book superheroes and fictional characters ever created. While Batman is sometimes paired with Superman,\n",
    "\n",
    "Isabel Cara\n",
    "\n",
    "00:08 / 00:20\n",
    "Batman is one of the most recognizable superheroes in pop culture, starting with his first appearance in Detective Comics #27 in 1939. He has been listed as one of the most iconic characters in popular culture, and among the greatest comic book superheroes and fictional characters ever created.\n",
    "\n",
    "While Batman is sometimes paired with Superman, Wonder Woman, and others from the Justice League, his stories have so much potential that they have been made into movies, tv shows, videogames, toy lines, radio, even musical theatre.\n",
    "\n",
    "You might find interesting: The Batman: Villains who would be exceptional candidates for the sequel\n",
    "\n",
    "ADVERTISING\n",
    "\n",
    "So, lets talk about how Batman, in his live-action version, has changed through the years:\n",
    "\n",
    "The Batman series (1966-1968), played by Adam West\n",
    "V2qn2wliqvchxfxv6w56zqm6se - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "This tv show (and a movie in 1966) portrays the character in a camp aesthetic, meaning that it could be appealing because of its bad taste and ironic value (like those so bad its good movies), but they camp things can also be described as cheesy.\n",
    "\n",
    "This aesthetic was associated with Batman for years after the show ended, and comic creators worked hard to return the character to his darker roots in the decades that followed, culminating in The Dark Knight Returns (1986) by Frank Miller.\n",
    "\n",
    "\n",
    "Nevertheless, this Batman holds a special place in every fan because references to this show have appeared in almost every Batman movie made after. With references appearing in media that is not related to the character, like the movie Ready Player One, and shows like The Simpsons, SpongeBob SquarePants, and The Fairly OddParents.\n",
    "\n",
    "While it might not be considered the best version of Batman, its the Batman that opened the doors to what we have now.\n",
    "\n",
    "Batman (1989), played by Michael Keaton\n",
    "6aheuzaikfealdjwgzy6366xt4 - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "This was a groundbreaking movie directed by Tim Burton, and starring Jack Nicholson, Michael Keaton, Kim Basinger, Robert Wuhl, Pat Hingle, Billy Dee Williams, Michael Gough, and Jack Palance.\n",
    "\n",
    "The movie was successful critically and financially, making it the 5th highest-grossing film in history (at the time of its release).\n",
    "\n",
    "It inspired Batman: The Animated Series (1992-1995), paving the way for the DC Animated Universe. The film has also influenced Hollywoods modern marketing and development techniques of the superhero film genre.\n",
    "\n",
    "It created the Batmania pop-culture phenomenon and made Batman cool again.\n",
    "\n",
    "Batman Forever (1995), played by Val Kilmer\n",
    "47zcqedjfrerpgasyogbpwjsvu - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "This movie strayed away from the aesthetic of Tim Burtons Batman, making it more colorful and over-the-top, especially with Jim Careys Ridley and Tommy Lee Joness Two-Face.\n",
    "\n",
    "It received mixed reviews from critics, but was a box office success, becoming the 6th highest-grossing film worldwide in 1995. Nevertheless, this movie tends to be ignored currently, with many fans not even remembering that Val Kilmer was Batman.\n",
    "\n",
    "Batman & Robin (1997), played by George Clooney\n",
    "3bulsal75ncwzny3xvd2nz33le - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "This film is regarded as the worst Batman movie and even getting spots as one of the worst movies ever made.\n",
    "\n",
    "\n",
    "It has become infamous because of the neon design, the Batnipples, and just being a franchise-killing movie (after being considered a box office disappointment, Warner Bros. decided to cancel future Batman films, like Schumachers -the director of this movie and Batman Forever- Batman Unchained).\n",
    "\n",
    "Its not even considered by fans so bad its good, and its almost always skipped in rewatches of the films.\n",
    "\n",
    "Batman Begins (2005), played by Christian Bale\n",
    "Doggryf3mbfklnwfyzr2t7vyoi - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "The most iconic Batman for the modern audience. Christopher Nolans work makes it one of the best superhero movies of all time.\n",
    "\n",
    "It rebooted the Batman film series, giving it a darker and more realistic tone compared to previous films (especially Schumachers job). The film wanted to create an emotional connection with the audience and the Batman and Bruce Wayne identities.\n",
    "\n",
    "The film became the 9th highest-grossing film of 2005 and was followed by The Dark Knight (2008), and The Dark Knight Rises (2012). Creating The Dark Knight Trilogy, and the reason many DC fans say that DC movies are better than Marvel Films.\n",
    "\n",
    "Batman v Superman: Dawn of Justice (2016), played by Ben Affleck\n",
    "R6tzxansdngnra5ee5izdeocva - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "Affleck is the only actor (so far) to not get his solo movie, and the possibilities of it happening are uncertain, maybe even impossible.\n",
    "\n",
    "Nevertheless, his role is different from the other Batmans, with him being older and jaded because of all the deaths and destruction he has seen but being hopeful in humanity among the member of the Justice League.\n",
    "\n",
    "He feels like Batman and Bruce are the same guys, and it really is a shame we couldnt spend more time with him.\n",
    "\n",
    "Gotham (2014-2019), played by David Mazouz\n",
    "Qf7qau37anf75nknc5wwobmnkm - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "Wait, who? Well yeah.\n",
    "\n",
    "The show started following James Gordon in his early days at the Gotham City Police Department following the tragic deaths of Bruces parents and started telling the origin stories of some of the most infamous Batman villains.\n",
    "\n",
    "During the fourth season, Bruce Wayne starts to fight crime as a masked vigilante, and in the fifth (and final) season it shows Bruce becoming Batman for the first time.\n",
    "\n",
    "\n",
    "While not the most popular version of Batman (I didnt even know the show had Batman in it until doing research for this article), it promises to give a good origin story, and a side of the character we havent seen before.\n",
    "\n",
    "The Batman (2022), played by Robert Pattinson\n",
    "Wu23mtvdhzbspep3rpmkrwm3fm - from adam west to robert pattinson: this is how batman has changed over the years\n",
    "This film, directed by Matt Reeves, is considered a reboot of the Batman film franchise (after the studio didnt know what to do with Ben Afflecks version).\n",
    "\n",
    "You might find interesting: Robert Pattinsons must watch films from Twilight to The Batman\n",
    "\n",
    "It tells the story of a young Batman, who has only been fighting crime for two years and is uncovering corruption while pursuing the Riddler, a serial killer who targets Gothams elite.\n",
    "\n",
    "Perhaps you have already seen it, or perhaps youre like me and havent been able to find the time. Either way, the memes for this movie and the use of Nirvanas Something in the Way have been incredible.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(text, window_size=4, stride=1)\n",
    "print(len(chunks))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2OOqDF5Kqas"
   },
   "outputs": [],
   "source": [
    "# pip install faiss-cpu     -> install faiss on cpu\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Get vector embeddings for the chunks\n",
    "vectors = np.array([get_vector_embeddings(chunk) for chunk in chunks])\n",
    "print(vectors.shape)\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "index.add(vectors)\n",
    "\n",
    "# Function to perform a vector search\n",
    "def vector_search(query_text, k=3):\n",
    "    query_vector = get_vector_embeddings(query_text)\n",
    "    distances, indices = index.search(np.array([query_vector]), k)\n",
    "    return [(chunks[i], float(dist)) for dist, i in zip(distances[0], indices[0])]\n",
    "\n",
    "# Example search\n",
    "search_results = vector_search(\"robert pattinson\")\n",
    "print(\"Search results for 'lorem ipsum':\")\n",
    "for result in search_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9Hlm2FMKyRy"
   },
   "outputs": [],
   "source": [
    "# Function to perform a vector search and then ask GPT-3.5-turbo a question\n",
    "def search_and_chat(chat_prompt, k=3):\n",
    "  # Perform the vector search\n",
    "    search_results = vector_search(chat_prompt, k)\n",
    "    print(f\"Search results: {search_results}\\n\\n\")\n",
    "\n",
    "    prompt_with_context = f\"\"\"Context:{search_results}\\\n",
    "    Answer the question: {chat_prompt}\"\"\"\n",
    "\n",
    "    # Create a list of messages for the chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please answer the questions provided by the user. Use only the context provided to you, if you don't know the answer say \\\"I don't know\\\".\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_with_context},\n",
    "    ]\n",
    "\n",
    "    # Get the model's response\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    # Print the assistant's reply\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "\n",
    "# Example search and chat\n",
    "search_and_chat(\"What song is associated with Robert Pattinson's Batman?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqzkYPARrUtBsmuHqCkBPe",
   "collapsed_sections": [
    "TQaI0BXpnGoM",
    "KfSdbZOZfV6e",
    "aORNVsNRwYkT",
    "n8ykU5VlifrF"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "368e095c405e474183d863d71279fe75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "4700f8c877c04c57b773d667f6f14ac5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9951535f583f44d0817da251d3cf6ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4700f8c877c04c57b773d667f6f14ac5",
      "placeholder": "",
      "style": "IPY_MODEL_e2c6949c87f0444084672d1416bf2f48",
      "tabbable": null,
      "tooltip": null,
      "value": "52155/52155[01:50&lt;00:00,586.54it/s]"
     }
    },
    "a451137b9ef04f94891b3aade3434816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2b6ca72b80e46848f2a56e615c3bbef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_f3bd05774b7b48ec9b675f98d5986cba",
      "max": 52155,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a451137b9ef04f94891b3aade3434816",
      "tabbable": null,
      "tooltip": null,
      "value": 52155
     }
    },
    "b3f335d9e3eb42209997dec316ae738d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb4337533a5c4de1bc47a2b0de944c70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf015341a6224e518f082f7f91c28d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b3f335d9e3eb42209997dec316ae738d",
      "placeholder": "",
      "style": "IPY_MODEL_368e095c405e474183d863d71279fe75",
      "tabbable": null,
      "tooltip": null,
      "value": "100%"
     }
    },
    "e2c6949c87f0444084672d1416bf2f48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "ef7409753e17486e96246c5dad2e4b03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf015341a6224e518f082f7f91c28d6d",
       "IPY_MODEL_b2b6ca72b80e46848f2a56e615c3bbef",
       "IPY_MODEL_9951535f583f44d0817da251d3cf6ded"
      ],
      "layout": "IPY_MODEL_bb4337533a5c4de1bc47a2b0de944c70",
      "tabbable": null,
      "tooltip": null
     }
    },
    "f3bd05774b7b48ec9b675f98d5986cba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
