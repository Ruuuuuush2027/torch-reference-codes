{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62bf7da5-936d-444b-849d-2a5788ba837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import re\n",
    "import os\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0758a532-620c-46cc-bca9-1c79e3b72d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run the create_train_test_from_paths.sh command to create train and test data\n",
    "NUM_WORKERS = 4\n",
    "torch.manual_seed(42)\n",
    "BATCH_SIZE = 256\n",
    "MODEL_SAVE_PATH = 'NRC_model_v1.pth'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49201d-c540-4d2e-ba76-23d87bb8b16f",
   "metadata": {},
   "source": [
    "### 1 - input data specification and verifciation\n",
    "1. note: transform function already defined in the parse_line; where each line is parsed\n",
    "2. input format requirement:\n",
    "\n",
    "| Index | Field | Sample Value |\n",
    "|-------|-------|-------|\n",
    "| 0 | contig | chr16 |\n",
    "| 1 | position | 46391349 |\n",
    "| 2 | reference_kmer | GGAATCGAG |\n",
    "| 3 | read_index | 1575 |\n",
    "| 4 | strand | t |\n",
    "| 5 | event_index | 215 |\n",
    "| 6 | event_level_mean | 120.07 |\n",
    "| 7 | event_stdv | 1.787 |\n",
    "| 8 | event_length | 0.00140 |\n",
    "| 9 | model_kmer | CTCGATTCC |\n",
    "| 10 | model_mean | 128.01 |\n",
    "| 11 | model_stdv | 5.05 |\n",
    "| 12 | standardized_level | -1.72 |\n",
    "| 13 | radiated | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8827a1a5-667d-41db-98aa-2df01d25ff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contig</th>\n",
       "      <th>position</th>\n",
       "      <th>reference_kmer</th>\n",
       "      <th>read_index</th>\n",
       "      <th>strand</th>\n",
       "      <th>event_index</th>\n",
       "      <th>event_level_mean</th>\n",
       "      <th>event_stdv</th>\n",
       "      <th>event_length</th>\n",
       "      <th>model_kmer</th>\n",
       "      <th>model_mean</th>\n",
       "      <th>model_stdv</th>\n",
       "      <th>standardized_level</th>\n",
       "      <th>radiated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>chr20</td>\n",
       "      <td>37273679</td>\n",
       "      <td>AGGCATGCG</td>\n",
       "      <td>2354</td>\n",
       "      <td>t</td>\n",
       "      <td>733</td>\n",
       "      <td>104.83</td>\n",
       "      <td>3.207</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>chr8</td>\n",
       "      <td>59383768</td>\n",
       "      <td>GAGAATGAA</td>\n",
       "      <td>4493</td>\n",
       "      <td>t</td>\n",
       "      <td>2135</td>\n",
       "      <td>77.14</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>chr2</td>\n",
       "      <td>160180051</td>\n",
       "      <td>GCAATGAGC</td>\n",
       "      <td>2150</td>\n",
       "      <td>t</td>\n",
       "      <td>3577</td>\n",
       "      <td>81.98</td>\n",
       "      <td>6.593</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>chr7</td>\n",
       "      <td>114496445</td>\n",
       "      <td>TGCATTATA</td>\n",
       "      <td>3914</td>\n",
       "      <td>t</td>\n",
       "      <td>2743</td>\n",
       "      <td>131.32</td>\n",
       "      <td>12.833</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>chr1</td>\n",
       "      <td>62773550</td>\n",
       "      <td>GGATGCTGC</td>\n",
       "      <td>125</td>\n",
       "      <td>t</td>\n",
       "      <td>5844</td>\n",
       "      <td>100.33</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875769</th>\n",
       "      <td>chr9</td>\n",
       "      <td>85495216</td>\n",
       "      <td>ATGCCACTT</td>\n",
       "      <td>4790</td>\n",
       "      <td>t</td>\n",
       "      <td>6736</td>\n",
       "      <td>66.14</td>\n",
       "      <td>1.392</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875824</th>\n",
       "      <td>chr4</td>\n",
       "      <td>182564933</td>\n",
       "      <td>TATAAATAT</td>\n",
       "      <td>3557</td>\n",
       "      <td>t</td>\n",
       "      <td>9639</td>\n",
       "      <td>130.51</td>\n",
       "      <td>5.776</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875834</th>\n",
       "      <td>chr3</td>\n",
       "      <td>102887639</td>\n",
       "      <td>ATCTGACTA</td>\n",
       "      <td>2774</td>\n",
       "      <td>t</td>\n",
       "      <td>568</td>\n",
       "      <td>115.38</td>\n",
       "      <td>11.024</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875847</th>\n",
       "      <td>chr2</td>\n",
       "      <td>206290671</td>\n",
       "      <td>TCAATAGAT</td>\n",
       "      <td>2200</td>\n",
       "      <td>t</td>\n",
       "      <td>1802</td>\n",
       "      <td>113.59</td>\n",
       "      <td>5.195</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875895</th>\n",
       "      <td>chr13</td>\n",
       "      <td>80487004</td>\n",
       "      <td>TTAAGATCA</td>\n",
       "      <td>1336</td>\n",
       "      <td>t</td>\n",
       "      <td>2540</td>\n",
       "      <td>62.27</td>\n",
       "      <td>1.185</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>NNNNNNNNN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16783 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contig   position reference_kmer  read_index strand  event_index  \\\n",
       "184     chr20   37273679      AGGCATGCG        2354      t          733   \n",
       "244      chr8   59383768      GAGAATGAA        4493      t         2135   \n",
       "263      chr2  160180051      GCAATGAGC        2150      t         3577   \n",
       "324      chr7  114496445      TGCATTATA        3914      t         2743   \n",
       "421      chr1   62773550      GGATGCTGC         125      t         5844   \n",
       "...       ...        ...            ...         ...    ...          ...   \n",
       "875769   chr9   85495216      ATGCCACTT        4790      t         6736   \n",
       "875824   chr4  182564933      TATAAATAT        3557      t         9639   \n",
       "875834   chr3  102887639      ATCTGACTA        2774      t          568   \n",
       "875847   chr2  206290671      TCAATAGAT        2200      t         1802   \n",
       "875895  chr13   80487004      TTAAGATCA        1336      t         2540   \n",
       "\n",
       "        event_level_mean  event_stdv  event_length model_kmer  model_mean  \\\n",
       "184               104.83       3.207        0.0010  NNNNNNNNN         0.0   \n",
       "244                77.14       0.754        0.0006  NNNNNNNNN         0.0   \n",
       "263                81.98       6.593        0.0016  NNNNNNNNN         0.0   \n",
       "324               131.32      12.833        0.0006  NNNNNNNNN         0.0   \n",
       "421               100.33       1.432        0.0012  NNNNNNNNN         0.0   \n",
       "...                  ...         ...           ...        ...         ...   \n",
       "875769             66.14       1.392        0.0018  NNNNNNNNN         0.0   \n",
       "875824            130.51       5.776        0.0006  NNNNNNNNN         0.0   \n",
       "875834            115.38      11.024        0.0008  NNNNNNNNN         0.0   \n",
       "875847            113.59       5.195        0.0010  NNNNNNNNN         0.0   \n",
       "875895             62.27       1.185        0.0014  NNNNNNNNN         0.0   \n",
       "\n",
       "        model_stdv  standardized_level  radiated  \n",
       "184            0.0                 inf         0  \n",
       "244            0.0                 inf         1  \n",
       "263            0.0                 inf         0  \n",
       "324            0.0                 inf         0  \n",
       "421            0.0                 inf         1  \n",
       "...            ...                 ...       ...  \n",
       "875769         0.0                 inf         1  \n",
       "875824         0.0                 inf         1  \n",
       "875834         0.0                 inf         0  \n",
       "875847         0.0                 inf         0  \n",
       "875895         0.0                 inf         1  \n",
       "\n",
       "[16783 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation: inf always appears together with NNNNNNNNN\n",
    "df = pd.read_csv('eval.tsv', delimiter='\\t')\n",
    "df[np.isinf(df.standardized_level)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdab637-76a7-485d-bf2d-058c9d0d5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the column matches the expectation given\n",
    "with open('train.tsv', 'r') as f:\n",
    "    first_line = f.readline().strip('\\n')\n",
    "    second_line = f.readline().strip('\\n')\n",
    "    \n",
    "    for idx, (i, j) in enumerate(zip(first_line.split('\\t'), second_line.split('\\t'))):\n",
    "        print(idx, '-', i, \":\", j)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044ac12-ae4f-42d3-a18a-9cbe4ad9cbfc",
   "metadata": {},
   "source": [
    "process note: \n",
    "1. one should use contig, but only parse the number directly behind chr\n",
    "2. discard read_index and event_index; read sequence should not be a criteria for judging performance\n",
    "3. standardized_level: inf always appear together with NNNNNN - I set it to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e7e5a-ffb1-46cd-94eb-9a5882b2a388",
   "metadata": {},
   "source": [
    "### 2 - define the iterable dataset for loading\n",
    "1. used iterable dataset so the entire kernel won't crush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145dd129-4a3e-4248-ac0e-1f2218d4dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the iterable dataset class\n",
    "class TsvIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self._length = None\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        parts = line.strip().split('\\t')\n",
    "        label = parts[-1]\n",
    "\n",
    "        features = [0]*24\n",
    "        # column 0-23 - one-hot encoding for chromosome number\n",
    "        chNum = re.search(r'chr(\\d+|X|Y)', parts[0])\n",
    "        if chNum is None:\n",
    "            pass\n",
    "        elif chNum.group(1) == 'X':\n",
    "            features[22] = 1\n",
    "        elif chNum.group(1) == 'Y':\n",
    "            features[23] = 1\n",
    "        else:\n",
    "            features[int(chNum.group(1)) - 1] = 1\n",
    "\n",
    "        # column 24 - position\n",
    "        features.append(int(parts[1]))\n",
    "\n",
    "        # column 25 - whether strand is t\n",
    "        features.append(1 if parts[4] == 't' else 0)\n",
    "\n",
    "        # column 26-30: from original column\n",
    "        features.append(float(parts[6]))\n",
    "        features.append(float(parts[7]))\n",
    "        features.append(float(parts[8]))\n",
    "        features.append(float(parts[10]))\n",
    "        features.append(float(parts[11]))\n",
    "\n",
    "        # column 31: if standardized level is inf, change to 0, better have minimal\n",
    "        # effect, as model kmer will document whether the transformation is valid\n",
    "        features.append(0 if parts[12]=='inf' else float(parts[12]))\n",
    "\n",
    "        # process kmer\n",
    "        idx_ref_kmer = 2\n",
    "        idx_model_kmer = 9\n",
    "        reference_kmer = parts[idx_ref_kmer]\n",
    "        model_kmer = parts[idx_model_kmer]\n",
    "        assert len(reference_kmer) == len(model_kmer), \"Kmers must be of the same length\"\n",
    "        assert len(model_kmer) == 9, \"must have 9 nucleotides for one-hot coding\"\n",
    "\n",
    "        # get kmers to onehot\n",
    "        nucleotide_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        ref_kmer_onehot=[]\n",
    "        for nucleotide in reference_kmer:\n",
    "            onehot = [0] * 4\n",
    "            if not(nucleotide == 'N'): # if N then neither of them\n",
    "                onehot[nucleotide_map[nucleotide]] = 1\n",
    "            ref_kmer_onehot.extend(onehot)\n",
    "        \n",
    "        model_kmer_onehot=[]\n",
    "        for nucleotide in reference_kmer:\n",
    "            onehot = [0] * 4\n",
    "            if not(nucleotide == 'N'): \n",
    "                onehot[nucleotide_map[nucleotide]] = 1\n",
    "            model_kmer_onehot.extend(onehot)\n",
    "\n",
    "        # column 32 - hamming distance; number positions in which models are different\n",
    "        hamming_distance = sum(c1 != c2 for c1, c2 in zip(reference_kmer, model_kmer))\n",
    "        features.append(hamming_distance)\n",
    "\n",
    "        # column 33-41 compute mismatch vector\n",
    "        mismatch_vector = [int(c1 != c2) for c1, c2 in zip(reference_kmer, model_kmer)]\n",
    "        features.extend(mismatch_vector)\n",
    "\n",
    "        # column 42-113 one hot encoding of the nucleotides; can discard if find to cumbersome\n",
    "        features.extend(ref_kmer_onehot)\n",
    "        features.extend(model_kmer_onehot)\n",
    "        \n",
    "        return torch.tensor(features), torch.tensor(int(label))\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            iter_start = 0\n",
    "            iter_end = None\n",
    "        else:\n",
    "            total_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id\n",
    "            iter_end = None \n",
    "\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            # skip header\n",
    "            next(f)\n",
    "            for idx, line in enumerate(f):\n",
    "                if worker_info is not None:\n",
    "                    if idx % worker_info.num_workers != worker_info.id:\n",
    "                        continue # skip lines not assigned to this worker\n",
    "                try: # if error, simply skip the line; some lines are not neatly parsed\n",
    "                    sample = self.parse_line(line)\n",
    "                except:\n",
    "                    continue;\n",
    "                yield sample\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._length is None:\n",
    "            self._length = self._count_lines()\n",
    "        return self._length\n",
    "\n",
    "    def _count_lines(self):\n",
    "        count = 0\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            next(f)  # skip header\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84706403-6082-4c22-9a97-ee117470e057",
   "metadata": {},
   "source": [
    "resulting column description (total = 114 features)\n",
    "| Index | Column Name | Description |\n",
    "|-------|-------------|-------------|\n",
    "| 0-23 | chromosome_number | Chromosome identification number |\n",
    "| 24 | position | Genomic position |\n",
    "| 25 | is_t_strand | Boolean indicating T-strand status |\n",
    "| 26 | event_level_mean | Mean of the event levels |\n",
    "| 27 | event_stdv | Standard deviation of the event levels |\n",
    "| 28 | event_length | Length of the event |\n",
    "| 29 | model_mean | Mean of the model |\n",
    "| 30 | model_stdv | Standard deviation of the model |\n",
    "| 31 | standardized_level | Normalized or standardized level |\n",
    "| 32 | model_reference_hamming_dist | Hamming distance to the model reference |\n",
    "| 33-41 | mismatch_vector | Binary vector indicating mismatches (1 if position mismatch) |\n",
    "| 42-113 | one-hot encoding | One-hot encoded vector representation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd64b60-05f2-4926-95ae-f929358f16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these paths to corresponding paths\n",
    "# if colab mount to drive!\n",
    "\n",
    "train_dataset = TsvIterableDataset('train.tsv')\n",
    "eval_dataset = TsvIterableDataset('eval.tsv')\n",
    "test_dataset = TsvIterableDataset('test.tsv')\n",
    "\n",
    "# the create_train_test_from_paths.sh shuffles the data on CMD level, so no need to do again\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb90b617-b290-41ce-846f-5227c6ff8d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### first three records in train_dataset: \n",
      "RECORD 0: (tensor([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.9031e+06, 1.0000e+00, 1.2911e+02, 1.4620e+00, 1.2000e-03, 1.2042e+02,\n",
      "        1.1570e+01, 7.8000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00]), tensor(1))\n",
      "RECORD 1: (tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        2.0328e+07, 1.0000e+00, 1.1068e+02, 9.2100e-01, 6.0000e-04, 1.0814e+02,\n",
      "        2.9300e+00, 1.0700e+00, 5.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]), tensor(1))\n",
      "RECORD 2: (tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.7730e+07, 1.0000e+00, 7.1510e+01, 5.7660e+00, 1.4000e-03, 6.9120e+01,\n",
      "        3.1800e+00, 9.4000e-01, 5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]), tensor(0))\n",
      "### first three records in eval_dataset:\n",
      "RECORD 0: (tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0280e+08, 1.0000e+00, 7.5580e+01, 6.6600e-01, 6.0000e-04, 7.4540e+01,\n",
      "        2.2900e+00, 5.7000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]), tensor(1))\n",
      "RECORD 1: (tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.6029e+08, 1.0000e+00, 9.9460e+01, 4.6800e-01, 6.0000e-04, 9.8050e+01,\n",
      "        3.3300e+00, 5.0000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00]), tensor(1))\n",
      "RECORD 2: (tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2616e+07,\n",
      "         1.0000e+00,  1.2387e+02,  1.3700e+00,  2.4000e-03,  1.2416e+02,\n",
      "         3.1900e+00, -1.1000e-01,  9.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         1.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00]), tensor(0))\n",
      "### first three records in test_dataset:\n",
      "RECORD 0: (tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.7005e+08, 1.0000e+00, 1.3386e+02, 6.6400e-01, 1.4000e-03, 1.2316e+02,\n",
      "        1.1850e+01, 1.0600e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]), tensor(1))\n",
      "RECORD 1: (tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1504e+07,\n",
      "         1.0000e+00,  1.1790e+02,  1.8540e+00,  8.0000e-04,  1.2031e+02,\n",
      "         3.7100e+00, -8.4000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]), tensor(1))\n",
      "RECORD 2: (tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.6484e+07, 1.0000e+00, 1.0914e+02, 2.6240e+00, 1.8000e-03, 1.0895e+02,\n",
      "        5.7600e+00, 4.0000e-02, 9.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]), tensor(0))\n",
      "### first iteration in train_loader: \n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0])\n",
      "torch.Size([256, 114]) torch.Size([256])\n",
      "### first iteration in eval_loader: \n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]]) tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1])\n",
      "torch.Size([256, 114]) torch.Size([256])\n",
      "### first iteration in test_loader: \n",
      "tensor([[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]]) tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1])\n",
      "torch.Size([256, 114]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# verify the dataset works, also define IN_FEATURES\n",
    "IN_FEATURES = 0\n",
    "\n",
    "print(\"### first three records in train_dataset: \")\n",
    "count = 0\n",
    "for row in train_dataset:\n",
    "    if count == 3:\n",
    "        IN_FEATURES = len(row[0])\n",
    "        break\n",
    "    print(f\"RECORD {count}:\", row)\n",
    "    count += 1\n",
    "\n",
    "print(\"### first three records in eval_dataset:\")\n",
    "count = 0\n",
    "for row in eval_dataset:\n",
    "    if count == 3:\n",
    "        break\n",
    "    print(f\"RECORD {count}:\", row)\n",
    "    count += 1\n",
    "\n",
    "print(\"### first three records in test_dataset:\")\n",
    "count = 0\n",
    "for row in test_dataset:\n",
    "    if count == 3:\n",
    "        break\n",
    "    print(f\"RECORD {count}:\", row)\n",
    "    count += 1\n",
    "\n",
    "# verify the loader works\n",
    "print('### first iteration in train_loader: ')\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs, labels)\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break\n",
    "\n",
    "print('### first iteration in eval_loader: ')\n",
    "for batch_idx, (inputs, labels) in enumerate(eval_loader):\n",
    "    print(inputs, labels)\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break\n",
    "\n",
    "print('### first iteration in test_loader: ')\n",
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "    print(inputs, labels)\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9561c39-7a08-4d78-801f-c5de7470af09",
   "metadata": {},
   "source": [
    "### 3 - define pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374e44c2-d9c0-4a5d-8907-37cc88b8a20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0716,  0.0777, -0.0219,  ...,  0.0177,  0.0289, -0.0874],\n",
       "                      [-0.0615, -0.0312,  0.0146,  ...,  0.0627,  0.0710,  0.0341],\n",
       "                      [-0.0653, -0.0924, -0.0761,  ..., -0.0102, -0.0082, -0.0222],\n",
       "                      ...,\n",
       "                      [-0.0205, -0.0923, -0.0866,  ..., -0.0467,  0.0651, -0.0009],\n",
       "                      [ 0.0377,  0.0169, -0.0124,  ..., -0.0689, -0.0100,  0.0828],\n",
       "                      [-0.0718,  0.0631,  0.0512,  ..., -0.0099, -0.0120, -0.0332]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.0152, -0.0532,  0.0547, -0.0267, -0.0534, -0.0274,  0.0343, -0.0535,\n",
       "                      -0.0874, -0.0212, -0.0635,  0.0916, -0.0329,  0.0237, -0.0441,  0.0023,\n",
       "                      -0.0206, -0.0137,  0.0776,  0.0394, -0.0475,  0.0484, -0.0603,  0.0721,\n",
       "                      -0.0827, -0.0913,  0.0919,  0.0512,  0.0033, -0.0724,  0.0578, -0.0380,\n",
       "                       0.0262,  0.0164,  0.0602, -0.0221,  0.0936, -0.0679, -0.0424,  0.0922,\n",
       "                      -0.0827, -0.0285,  0.0293,  0.0375, -0.0299, -0.0735,  0.0792, -0.0719,\n",
       "                       0.0892,  0.0070, -0.0535,  0.0131, -0.0031, -0.0780,  0.0813, -0.0547,\n",
       "                      -0.0167, -0.0502,  0.0206, -0.0628,  0.0803, -0.0249,  0.0874,  0.0004,\n",
       "                      -0.0151, -0.0840,  0.0588, -0.0006, -0.0745,  0.0452,  0.0142, -0.0512,\n",
       "                      -0.0563, -0.0189,  0.0439,  0.0302, -0.0579, -0.0657,  0.0784,  0.0183,\n",
       "                       0.0457, -0.0883,  0.0442,  0.0813, -0.0029,  0.0760,  0.0198,  0.0037,\n",
       "                       0.0327, -0.0328, -0.0099, -0.0183,  0.0242,  0.0093, -0.0437,  0.0480,\n",
       "                      -0.0525,  0.0591,  0.0094, -0.0060, -0.0061,  0.0205,  0.0039,  0.0817,\n",
       "                       0.0750,  0.0931,  0.0569,  0.0020, -0.0363, -0.0348,  0.0568, -0.0074,\n",
       "                      -0.0840,  0.0029, -0.0221, -0.0678, -0.0710, -0.0086,  0.0335,  0.0505,\n",
       "                      -0.0671, -0.0792, -0.0715, -0.0038, -0.0077,  0.0437, -0.0762,  0.0852],\n",
       "                     device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.0265, -0.0837, -0.0557,  ..., -0.0525, -0.0109, -0.0863],\n",
       "                      [-0.0690,  0.0709, -0.0648,  ..., -0.0590, -0.0667, -0.0522],\n",
       "                      [-0.0150, -0.0454,  0.0379,  ...,  0.0189,  0.0328, -0.0621],\n",
       "                      ...,\n",
       "                      [-0.0109,  0.0114, -0.0595,  ..., -0.0622, -0.0131,  0.0312],\n",
       "                      [-0.0884, -0.0132, -0.0202,  ...,  0.0640, -0.0873, -0.0840],\n",
       "                      [ 0.0750, -0.0360,  0.0169,  ..., -0.0784,  0.0723,  0.0695]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0707,  0.0420, -0.0615, -0.0465, -0.0373,  0.0245,  0.0476, -0.0042,\n",
       "                      -0.0870, -0.0560, -0.0423, -0.0595,  0.0455,  0.0039, -0.0631, -0.0536,\n",
       "                      -0.0646, -0.0612,  0.0331, -0.0464, -0.0757,  0.0064, -0.0053, -0.0733,\n",
       "                       0.0238,  0.0075,  0.0115,  0.0388, -0.0446, -0.0772,  0.0571,  0.0802],\n",
       "                     device='cuda:0')),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.1642,  0.0943,  0.1625,  0.0499, -0.1542,  0.0526, -0.1352, -0.1665,\n",
       "                        0.0684, -0.0936, -0.1277,  0.1669,  0.0563,  0.0755, -0.0528,  0.0661,\n",
       "                       -0.0028, -0.0413, -0.0337,  0.1312, -0.1209, -0.0545,  0.1291, -0.0082,\n",
       "                        0.0868, -0.0749,  0.1186, -0.1257, -0.0948, -0.1555, -0.1412,  0.1113]],\n",
       "                     device='cuda:0')),\n",
       "             ('output.bias', tensor([0.1143], device='cuda:0'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v0 does not have batchnorm\n",
    "# stuck at 54.7399, vanishing gradient\n",
    "class NanoporeRadClassifer_v0(nn.Module):\n",
    "    # \n",
    "    def __init__(self, in_features=IN_FEATURES):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        res = self.layer1(x)\n",
    "        res = self.relu(res)\n",
    "        res = self.layer2(res)\n",
    "        res = self.relu(res)\n",
    "        return self.output(res)\n",
    "\n",
    "NRC_model_v0 = NanoporeRadClassifer_v0().to(device)\n",
    "# NRC_model_v0 = torch.compile(NRC_model_v0)\n",
    "NRC_model_v0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be2f3de-d514-45a2-99bc-a040c97dd205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.0456,  0.0550,  0.0826,  ..., -0.0824, -0.0404, -0.0561],\n",
       "                      [ 0.0003, -0.0349, -0.0065,  ...,  0.0698,  0.0450,  0.0788],\n",
       "                      [ 0.0491,  0.0237, -0.0009,  ..., -0.0502,  0.0905, -0.0452],\n",
       "                      ...,\n",
       "                      [-0.0799, -0.0709,  0.0845,  ..., -0.0398, -0.0199, -0.0247],\n",
       "                      [-0.0770,  0.0318,  0.0370,  ...,  0.0123,  0.0616,  0.0463],\n",
       "                      [ 0.0297,  0.0133, -0.0609,  ..., -0.0267, -0.0534, -0.0274]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.0343, -0.0535, -0.0874, -0.0212, -0.0635,  0.0916, -0.0329,  0.0237,\n",
       "                      -0.0441,  0.0023, -0.0206, -0.0137,  0.0776,  0.0394, -0.0475,  0.0484,\n",
       "                      -0.0603,  0.0721, -0.0827, -0.0913,  0.0919,  0.0512,  0.0033, -0.0724,\n",
       "                       0.0578, -0.0380,  0.0262,  0.0164,  0.0602, -0.0221,  0.0936, -0.0679,\n",
       "                      -0.0424,  0.0922, -0.0827, -0.0285,  0.0293,  0.0375, -0.0299, -0.0735,\n",
       "                       0.0792, -0.0719,  0.0892,  0.0070, -0.0535,  0.0131, -0.0031, -0.0780,\n",
       "                       0.0813, -0.0547, -0.0167, -0.0502,  0.0206, -0.0628,  0.0803, -0.0249,\n",
       "                       0.0874,  0.0004, -0.0151, -0.0840,  0.0588, -0.0006, -0.0745,  0.0452,\n",
       "                       0.0142, -0.0512, -0.0563, -0.0189,  0.0439,  0.0302, -0.0579, -0.0657,\n",
       "                       0.0784,  0.0183,  0.0457, -0.0883,  0.0442,  0.0813, -0.0029,  0.0760,\n",
       "                       0.0198,  0.0037,  0.0327, -0.0328, -0.0099, -0.0183,  0.0242,  0.0093,\n",
       "                      -0.0437,  0.0480, -0.0525,  0.0591,  0.0094, -0.0060, -0.0061,  0.0205,\n",
       "                       0.0039,  0.0817,  0.0750,  0.0931,  0.0569,  0.0020, -0.0363, -0.0348,\n",
       "                       0.0568, -0.0074, -0.0840,  0.0029, -0.0221, -0.0678, -0.0710, -0.0086,\n",
       "                       0.0335,  0.0505, -0.0671, -0.0792, -0.0715, -0.0038, -0.0077,  0.0437,\n",
       "                      -0.0762,  0.0852, -0.0280, -0.0887, -0.0591,  0.0227, -0.0194,  0.0678],\n",
       "                     device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.0004,  0.0538,  0.0733,  ...,  0.0795,  0.0457,  0.0817],\n",
       "                      [-0.0414, -0.0693,  0.0735,  ...,  0.0764,  0.0875,  0.0598],\n",
       "                      [ 0.0365,  0.0010, -0.0472,  ...,  0.0633, -0.0122, -0.0284],\n",
       "                      ...,\n",
       "                      [ 0.0247, -0.0104,  0.0558,  ..., -0.0385,  0.0366,  0.0619],\n",
       "                      [ 0.0107, -0.0338,  0.0278,  ...,  0.0312,  0.0203, -0.0456],\n",
       "                      [-0.0602, -0.0684,  0.0365,  ..., -0.0465, -0.0373,  0.0245]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0476, -0.0042, -0.0870, -0.0560, -0.0423, -0.0595,  0.0455,  0.0039,\n",
       "                      -0.0631, -0.0536, -0.0646, -0.0612,  0.0331, -0.0464, -0.0757,  0.0064,\n",
       "                      -0.0053, -0.0733,  0.0238,  0.0075,  0.0115,  0.0388, -0.0446, -0.0772,\n",
       "                       0.0571,  0.0802,  0.0821,  0.0472,  0.0812,  0.0249, -0.0771,  0.0263],\n",
       "                     device='cuda:0')),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.1352, -0.1665,  0.0684, -0.0936, -0.1277,  0.1669,  0.0563,  0.0755,\n",
       "                       -0.0528,  0.0661, -0.0028, -0.0413, -0.0337,  0.1312, -0.1209, -0.0545,\n",
       "                        0.1291, -0.0082,  0.0868, -0.0749,  0.1186, -0.1257, -0.0948, -0.1555,\n",
       "                       -0.1412,  0.1113,  0.1143,  0.0948,  0.0875,  0.1199,  0.1075,  0.1040]],\n",
       "                     device='cuda:0')),\n",
       "             ('output.bias', tensor([-0.0868], device='cuda:0')),\n",
       "             ('bn1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.], device='cuda:0')),\n",
       "             ('bn1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('bn1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('bn1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.], device='cuda:0')),\n",
       "             ('bn1.num_batches_tracked', tensor(0, device='cuda:0')),\n",
       "             ('bn2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('bn2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('bn2.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('bn2.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('bn2.num_batches_tracked', tensor(0, device='cuda:0'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v0 does not have dropout\n",
    "class NanoporeRadClassifer_v1(nn.Module):\n",
    "    def __init__(self, in_features=IN_FEATURES):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=32)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        res = self.layer1(x)\n",
    "        res = self.bn1(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.layer2(res)\n",
    "        res = self.bn2(res)\n",
    "        res = self.relu(res)\n",
    "        return self.output(res)\n",
    "\n",
    "NRC_model_v1 = NanoporeRadClassifer_v1().to(device)\n",
    "# NRC_model_v0 = torch.compile(NRC_model_v0)\n",
    "NRC_model_v1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb532799-e361-4090-9b7f-04d8a9274b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert linear output to actual prediction\n",
    "def to_prediction(model_output, threshold = .5):\n",
    "    return torch.sigmoid(model_output) >= threshold\n",
    "\n",
    "# define accuracy_score for evaluation\n",
    "def accuracy_score(y_pred, y_true, threshold = .5):\n",
    "    correct = torch.eq(to_prediction(y_pred, threshold), y_true)\n",
    "    return correct.sum().item() / len(y_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e0db5a-3054-4741-86b8-2d3aaafdcc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_28171691/ipykernel_28133/2167395084.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n"
     ]
    }
   ],
   "source": [
    "# define loss, optimizer; don't forget to change model type when training\n",
    "# change this one for different models\n",
    "model = NRC_model_v1\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(),\n",
    "                             lr = .01)\n",
    "\n",
    "epochs = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "eval_losses = []\n",
    "eval_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97411ee7-a167-45b6-992f-c3880addc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 114]) torch.Size([256])\n",
      "256\n",
      "first five outputs:  tensor([[0.1817],\n",
      "        [0.1817],\n",
      "        [0.1817],\n",
      "        [0.1817],\n",
      "        [0.1817]], device='cuda:0')\n",
      "first five predictions from outputs:  tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]], device='cuda:0')\n",
      "raw accuracy:  57.421875\n"
     ]
    }
   ],
   "source": [
    "# do a sample run to ensure model works\n",
    "for batch_idx, (sample_input, sample_labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "print(sample_input.shape, sample_labels.shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    untrained_preds = model(sample_input.to(device))\n",
    "\n",
    "print(len(untrained_preds))\n",
    "print('first five outputs: ', untrained_preds[:5])\n",
    "print('first five predictions from outputs: ', to_prediction(untrained_preds[:5]))\n",
    "print('raw accuracy: ', accuracy_score(untrained_preds, sample_labels.unsqueeze(dim=1).to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042f5b-f7ec-442e-9cb6-14ccafc832e3",
   "metadata": {},
   "source": [
    "### 4 - Training\n",
    "1. IMPORTANT: when using labels from each iteration, do labels = labels.unsqueeze(dim=1) so that the dimensionality matches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cab7527-2cb3-4a26-a253-98216f40ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97915258-fae5-453b-87c8-040e97c54e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 0.6889 | eval_loss = 0.6891\n",
      "average train accuracy: 54.6649\n",
      "average eval accuracy: 54.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_loss = 0.6889 | eval_loss = 0.6891\n",
      "average train accuracy: 54.6649\n",
      "average eval accuracy: 54.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train_loss = 0.6889 | eval_loss = 0.6891\n",
      "average train accuracy: 54.6649\n",
      "average eval accuracy: 54.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m save_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m     11\u001b[0m update_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_tqdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/multiprocessing/reductions.py:541\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 541\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py:526\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     answer_challenge(c, authkey)\n\u001b[0;32m--> 526\u001b[0m     \u001b[43mdeliver_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py:939\u001b[0m, in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey, digest_name)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# Even when sending a challenge to a legacy client that does not support\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# digest prefixes, they'll take the entire thing as a challenge and\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# respond to it with a raw HMAC-MD5.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m connection\u001b[38;5;241m.\u001b[39msend_bytes(_CHALLENGE \u001b[38;5;241m+\u001b[39m message)\n\u001b[0;32m--> 939\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     _verify_challenge(authkey, message, response)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epochs.append(epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    average_train_accuracy = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} - Training\", leave=False)\n",
    "    save_interval = int(len(train_loader) * 0.3)\n",
    "    update_interval = max(1, len(train_loader) // 100)\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader_tqdm):\n",
    "        if (batch_idx + 1) % save_interval == 0:\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        X_train = inputs.to(device)\n",
    "        y_train = labels.unsqueeze(dim=1).float().to(device)\n",
    "        y_train_pred = model(X_train)\n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        batch_train_accuracy = accuracy_score(y_train_pred, y_train)\n",
    "        average_train_accuracy += batch_train_accuracy\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % update_interval == 0:\n",
    "            train_loader_tqdm.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{batch_train_accuracy:.4f}\"\n",
    "            })\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    average_train_accuracy /= len(train_loader)\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    average_eval_accuracy = 0\n",
    "    update_interval = max(1, len(eval_loader) // 100)\n",
    "    eval_loader_tqdm = tqdm(eval_loader, desc=f\"Epoch {epoch}/{EPOCHS} - Evaluating\", leave=False)\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, labels) in enumerate(eval_loader_tqdm):\n",
    "            X_eval = inputs.to(device)\n",
    "            y_eval = labels.unsqueeze(dim=1).float().to(device)\n",
    "            y_eval_pred = model(X_eval)\n",
    "            loss = loss_fn(y_eval_pred, y_eval)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            batch_eval_accuracy = accuracy_score(y_eval_pred, y_eval)\n",
    "            average_eval_accuracy += batch_eval_accuracy\n",
    "            if batch_idx % update_interval == 0:\n",
    "                eval_loader_tqdm.set_postfix({\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                    \"Acc\": f\"{batch_eval_accuracy:.4f}\"\n",
    "                })\n",
    "            \n",
    "    eval_loss /= len(eval_loader)\n",
    "    average_eval_accuracy /= len(eval_loader)\n",
    "            \n",
    "    print(f'epoch {epoch}: train_loss = {train_loss:.4f} | eval_loss = {eval_loss:.4f}')\n",
    "    # accuracy_score takes y_train_pred to logit\n",
    "    print(f'average train accuracy: {average_train_accuracy:.4f}')\n",
    "    print(f'average eval accuracy: {average_eval_accuracy:.4f}')\n",
    "\n",
    "    # save result after every epoch\n",
    "    # train_history = {\"epochs\":epochs, \"train_losses\": train_losses, \"train_accuracies\": train_accuracies, \n",
    "    # \"eval_losses\": eval_losses, \"eval_accuracies\": eval_accuracies}\n",
    "    train_history = {}\n",
    "    with open(\"train_history.json\", \"r\") as f:\n",
    "        train_history = json.loads(f.read())\n",
    "    train_history['epochs'].extend(epochs)\n",
    "    train_history['train_losses'].append(train_loss)\n",
    "    train_history['train_accuracies'].append(average_train_accuracy)\n",
    "    train_history['eval_losses'].append(eval_loss)\n",
    "    train_history['eval_accuracies'].append(average_eval_accuracy)\n",
    "    with open(\"train_history.json\", \"w\") as f:\n",
    "        json.dump(train_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e1111-3afc-4840-938d-a9d85458358b",
   "metadata": {},
   "source": [
    "### 5 - Evaluation\n",
    "1. plot the history plots by epoch\n",
    "2. evaluate testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b68e8-f5a5-48d5-aa06-3a093f23528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NanoporeRadClassifer_v0()\n",
    "# .load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785972b2-767d-46eb-92ac-608112754ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be implemented: plot history\n",
    "with open(\"my_list.json\", \"r\") as f:\n",
    "    train_history = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205a9e4-bdc7-496f-990d-c5300a80ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "average_test_accuracy = 0\n",
    "\n",
    "test_loader_tqdm = tqdm(test_loader, desc=f\"Epoch {epoch}/{EPOCHS} - Evaluating\", leave=False)\n",
    "update_interval = max(1, len(test_loader) // 100)\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_loader_tqdm):\n",
    "        X_test = inputs.to(device)\n",
    "        y_test = labels.unsqueeze(dim=1).float().to(device)\n",
    "        y_test_pred = model(X_test)\n",
    "        loss = loss_fn(y_test_pred, y_test)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        batch_test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "        average_test_accuracy += batch_test_accuracy\n",
    "        if batch_idx % update_interval == 0:\n",
    "            test_loader_tqdm.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{batch_test_accuracy:.4f}\"\n",
    "            })\n",
    "        \n",
    "test_loss /= len(test_loader)\n",
    "average_test_accuracy /= len(test_loader)\n",
    "\n",
    "print(f'TEST: test_loss = {test_loss:.4f} | average_test_accuracy = {average_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b8688e-56b2-4903-bc5d-d4e19b99d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: test_loss = 0.6887 | average_eval_accuracy = 54.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# eval loss\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "average_test_accuracy = 0\n",
    "\n",
    "test_loader_tqdm = tqdm(eval_loader, desc=f\"Evaluating\", leave=False)\n",
    "update_interval = max(1, len(eval_loader) // 100)\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_loader_tqdm):\n",
    "        X_test = inputs.to(device)\n",
    "        y_test = labels.unsqueeze(dim=1).float().to(device)\n",
    "        y_test_pred = model(X_test)\n",
    "        loss = loss_fn(y_test_pred, y_test)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        batch_test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "        average_test_accuracy += batch_test_accuracy\n",
    "\n",
    "        if batch_idx % update_interval == 0:\n",
    "            test_loader_tqdm.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{batch_test_accuracy:.4f}\"\n",
    "            })\n",
    "        \n",
    "test_loss /= len(eval_loader)\n",
    "average_test_accuracy /= len(eval_loader)\n",
    "\n",
    "print(f'TEST: test_loss = {test_loss:.4f} | average_eval_accuracy = {average_test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch enabled kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
